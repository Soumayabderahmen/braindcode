from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from langdetect import detect
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema.document import Document
from langchain_community.llms import Ollama
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_ollama import OllamaEmbeddings
from langchain_ollama import OllamaLLM as Ollama
from fastapi.responses import StreamingResponse
from fastapi import UploadFile, File
from langchain_community.document_loaders import PyPDFLoader
from fastapi import Body
from symspellpy import SymSpell, Verbosity
from textblob import TextBlob
from fuzzywuzzy import process
import os
import tempfile
import os
import httpx
import logging
import time
import re
import json
import asyncio
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def load_existing_user_vectors():
    base_path = "./user_vectors"
    if not os.path.exists(base_path):
        os.makedirs(base_path)

    for session_id in os.listdir(base_path):
        dir_path = os.path.join(base_path, session_id)
        if os.path.isdir(dir_path):
            try:
                user_pdf_vectors[session_id] = Chroma(
                    embedding_function=embedding_model,
                    persist_directory=dir_path
                )
                logging.info(f"[Persistence] Vecteur charg√© pour session : {session_id}")
            except Exception as e:
                logging.warning(f"[Persistence] √âchec de chargement vecteur session {session_id}: {e}")

logging.basicConfig(level=logging.INFO)
# Initialisation de l'embedding + LLM
embedding_model = OllamaEmbeddings(model="nomic-embed-text")
llm = Ollama(model="llama3")




# Cr√©ation du vecteurstore √† partir du fichier intentions.txt
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
intentions_path = "intentions/intentions.txt"
# Stockage temporaire en RAM par utilisateur
#user_pdf_vectors = {}
#  Charger les vecteurs utilisateurs sauvegard√©s
#load_existing_user_vectors()


# @app.post("/upload-user-pdf")
# async def upload_user_pdf(request: Request, file: UploadFile = File(...)):
#     session_id = request.headers.get("X-Session-ID", "anonymous")
#     logging.info(f"[Upload PDF] session_id = {session_id}")  # pour assurer que session_id est bien pris en compte 
#     if not file.filename.endswith(".pdf"):
#         return JSONResponse(status_code=400, content={"error": "Fichier non PDF."})

#     with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
#         tmp.write(await file.read())
#         tmp_path = tmp.name

#     loader = PyPDFLoader(tmp_path)
#     documents = loader.load()

#     if not documents:
#         return JSONResponse(status_code=400, content={"error": "PDF vide ou illisible."})

#     splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
#     texts = splitter.split_documents(documents)

#     if not texts:
#         return JSONResponse(status_code=400, content={"error": "Aucun texte d√©tect√©."})

#     persist_dir = f"./user_vectors/{session_id}"
#     vectordb = Chroma.from_documents(texts, embedding_model, persist_directory=persist_dir)
#     vectordb.persist()  # Sauvegarde sur disque
#     user_pdf_vectors[session_id] = vectordb

#     return {"message": "Fichier PDF analys√© avec succ√®s."}


def create_intentions_db():
    loader = TextLoader(intentions_path)
    raw_docs = loader.load()
    
    documents = []
    current_intent = None

    for doc in raw_docs:
        lines = doc.page_content.splitlines()
        for line in lines:
            line = line.strip()
            if line.startswith("#INTENTION:"):
                current_intent = line.split(":", 1)[1].strip()
            elif line and current_intent is not None:
                documents.append(Document(
                    page_content=line,
                    metadata={"intent_name": str(current_intent)}  # ‚úÖ √©vite None
                ))

    texts = text_splitter.split_documents(documents)

# üîç V√©rification du contenu avant insertion
    if not texts:
        logging.error("[Intentions DB] Aucun document √† indexer. V√©rifiez intentions.txt.")
        raise ValueError("Aucun texte √† vectoriser.")

# ‚úÖ Filtrer les documents vides
    texts = [doc for doc in texts if doc.page_content.strip()]

# üîç Rejet si toujours vide
    if not texts:
        raise ValueError("Tous les documents sont vides apr√®s nettoyage.")

# ‚úÖ Cr√©ation du vecteurstore
    return Chroma.from_documents(texts, embedding_model, persist_directory="./chroma-intentions")


   





def detect_language(text):
    try:
        # Supprimer les emojis
        cleaned = re.sub(r'[^\w\s,.\'‚Äô!?-]', '', text)
        return detect(cleaned)
    except:
        return "unknown"


# Initialiser SymSpell (√† faire une seule fois)
# Initialisation de SymSpell
sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)
sym_spell.load_dictionary("data/french_dictionary.txt", term_index=0, count_index=1)

# Liste des mots du dico (pour fuzzywuzzy)
with open("data/french_dictionary.txt", encoding="utf-8") as f:
    DICTIONARY_WORDS = [line.split()[0] for line in f.readlines()]

def corriger_fautes(message: str) -> str:
    # 1. Correction mot √† mot avec SymSpell
    mots = message.strip().split()
    message_symspell = []

    for mot in mots:
        suggestions = sym_spell.lookup(mot, Verbosity.CLOSEST, max_edit_distance=2)
        if suggestions:
            message_symspell.append(suggestions[0].term)
        else:
            message_symspell.append(mot)

    phrase_corrigee = " ".join(message_symspell)

    # 2. Correction grammaticale avec TextBlob
    try:
        phrase_blob = str(TextBlob(phrase_corrigee).correct())
    except Exception:
        phrase_blob = phrase_corrigee

    # 3. Dernier recours : fuzzywuzzy pour les mots douteux
    mots_finals = []
    for mot in phrase_blob.split():
        meilleur_match, score = process.extractOne(mot, DICTIONARY_WORDS)
        if score > 90:
            mots_finals.append(meilleur_match)
        else:
            mots_finals.append(mot)

    return " ".join(mots_finals)

def generate_language_sensitive_prompt(message: str) -> str:
    try:
        lang = detect(message)
    except Exception:
        lang = "unknown"

    if lang == "fr":
        return (
            "Tu es un assistant IA sympathique. Si quelqu‚Äôun √©crit 'bonjour' ou 'salut', "
            "r√©ponds simplement par une salutation naturelle comme 'Bonjour ! üòä', 'Salut !', ou autre. "
            f"Voici le message utilisateur : {message}"
        )
    elif lang == "en":
        return (
            "You are a friendly AI assistant. If the user says 'hello' or 'hi', "
            "just respond with a natural greeting like 'Hey there!', 'Hello! üòä', or something similar. "
            f"Here is the user message: {message}"
        )
    else:
        return f"You are a helpful assistant. Respond simply and naturally to the user message: {message}"

def detect_intention_semantic(message: str):
    try:
        results = intentions_db.similarity_search_with_score(message, k=1)
        if not results:
            logging.info("[Intentions] Aucun r√©sultat trouv√©.")
            return None, None
        
        best_match: tuple[Document, float] = results[0]
        doc, score = best_match

        logging.info(f"[Intentions] Similarit√©: {score:.4f}")
        if score > 0.3:
            return None, None  # Trop √©loign√©

        # R√©cup√®re le nom de l‚Äôintention depuis les metadata
        intent_name = doc.metadata.get("intent_name", "unknown")
        return intent_name, doc.page_content.strip()

    except Exception as e:
        logging.error(f"[Intentions ERROR] {e}")
        return None, None

@app.get("/intentions/list")
async def list_intentions():
    try:
        results = intentions_db.similarity_search("liste", k=20)
        return [doc.metadata.get("intent_name", "unknown") for doc in results]
    except Exception as e:
        return JSONResponse(status_code=500, content={"error": str(e)})


@app.post("/chat")
async def chat(request: Request):
    data = await request.json()
    intent_override = data.get("intent_override")  # ‚úÖ ajout
    message = data.get("message", "").strip()
    history = data.get("history", [])  # üëà Historique du frontend

    if len(message) > 1000:
        return JSONResponse(
            status_code=400,
            content={"error": "Message trop long (1000 caract√®res max autoris√©s)."}
        )

    message = corriger_fautes(message)
    lang = detect_language(message)
    if intent_override:
        intent_name = intent_override

        # üî• Forcer la langue fran√ßaise si c'est une intention connue
        if intent_name in {
            "liste_etapes_programme",
            "objectif_du_programme",
            "contact_mentor",
            "prise_de_rdv",
            "besoin_aide_financement",
            "livrable",
            "retard"
        }:
            lang = "fr"
        else:
            lang = detect_language(message)

        prompt_from_intent = build_personalized_prompt(intent_name, message, lang, "")
    else:
        intent_name, prompt_from_intent = detect_intention_semantic(message)





    # Nettoyage du message utilisateur
    cleaned_message = re.sub(r'\s+', ' ', message).strip()[:500]
    cleaned_message = re.sub(r"[^a-zA-Z√Ä-√ø0-9\s]", "", cleaned_message).lower()
    logging.info(f"[Message utilisateur] {cleaned_message}")

    # üîÅ Traitement de l'historique en texte structur√©
    formatted_history = []
    for item in history:
        if isinstance(item, dict) and "sender" in item and "text" in item:
            sender = "User" if item["sender"] == "user" else "Bot"
            text = item["text"].strip()
            formatted_history.append(f"{sender}: {text}")

    if len(formatted_history) > 10:
        prompt_history = "\n".join(formatted_history[-10:])
    else:
        prompt_history = "\n".join(formatted_history)

    # R√©cup√©ration du session_id
    session_id = request.headers.get("X-Session-ID", "anonymous")
    logging.info(f"[Chat] session_id = {session_id}")

    prompt_to_send = None

    # # ‚úÖ Priorit√© 1 : PDF utilisateur
    # if session_id in user_pdf_vectors:
    #     try:
    #         pdf_search = user_pdf_vectors[session_id].similarity_search_with_score(cleaned_message, k=2)
    #         chunks = [doc.page_content for doc, score in pdf_search if score < 0.5]
    #         if chunks:
    #             joined_chunks = "\n".join(chunks)
    #             prompt_to_send = (
    #                 f"Voici un extrait du fichier envoy√© :\n{joined_chunks}\n\n"
    #                 f"Question utilisateur : {cleaned_message}\n\n"
    #                 f"R√©ponds pr√©cis√©ment, en 500 caract√®res max."
    #             )
    #     except Exception as e:
    #         logging.warning(f"[User PDF Search Error] {e}")

    # ‚úÖ Priorit√© 2 : Intentions
    if not prompt_to_send and intent_name and prompt_from_intent:
        prompt_to_send = prompt_from_intent

    # ‚úÖ Priorit√© 3 : Fallback LLM avec historique
    if not prompt_to_send:
        # R√©duire la taille de l'historique √†  √©changes max (6 messages total)
        reduced_history = formatted_history[-6:]  # 3 user + 3 bot max
        prompt_history = "\n".join(reduced_history)

        # Si encore trop long (au cas o√π), tronque par caract√®res
        if len(prompt_history) > 1000:
            prompt_history = prompt_history[-1000:]

        #G√©n√®re le prompt personnalis√©
        prompt_base = build_personalized_prompt(intent_name, cleaned_message, lang, prompt_history)

        #Assemble le prompt final
        prompt_to_send = f"{prompt_base}"  # ‚úÖ ici



    

    # ‚úÖ Prompt final
    final_prompt = prompt_to_send

    logging.info(f"[Prompt final envoy√© √† Ollama] {final_prompt}")

    return StreamingResponse(stream_generator(final_prompt), media_type="text/plain")






def build_personalized_prompt(intent_name: str, user_message: str, lang: str, context_dialogue: str) -> str:
    if lang == "fr":
        base = "Tu es BrainBot, l‚Äôassistant IA du programme d‚Äôincubation BraindCode Startup Studio."
    else:
        base = "You are BrainBot, the AI assistant of the BrainCode startup program."

    # Ajout de l'historique de conversation au prompt
    if context_dialogue:
        base += f"\n\nVoici l‚Äôhistorique de la conversation r√©cente :\n{context_dialogue}"

    if intent_name == "salutation":
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "R√©ponds par une salutation amicale et chaleureuse."
        )

    elif intent_name == "contact_mentor":
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "Pour contacter votre coach ou mentor, merci d'utiliser votre espace personnel sur la plateforme BrainCode.\n"
            "üëâ Cliquez ici : https://braincode.tn/dashboard/coach\n"
            "Depuis cette section, vous pouvez envoyer une demande facilement.\n"
            "Bonne chance dans votre parcours entrepreneurial üöÄ"
        )

    elif intent_name == "liste_etapes_programme":
        return (
            f"{base}\n\n"
            f"L‚Äôutilisateur souhaite conna√Ætre les 12 √©tapes du programme BrainCode.\n"
            f"R√©ponds avec une liste num√©rot√©e tr√®s claire. Chaque √©tape doit contenir uniquement un titre explicite.\n\n"
            f"Format attendu :\n"
            f"1. Titre de l'√©tape 1\n"
            f"2. Titre de l'√©tape 2\n"
            f"...\n"
            f"12. Titre de l'√©tape 12\n\n"
            f"Exclusivement la liste. Pas d‚Äôintroduction, pas de conclusion, pas de texte inutile.\n\n"
            f"Message utilisateur : {user_message}"
    )


    elif intent_name == "objectif_du_programme":
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "Explique l‚Äôobjectif g√©n√©ral du programme d‚Äôincubation BrainCode de mani√®re concise et motivante."
        )

    elif intent_name == "livrable":
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "Indique que chaque √©tape a un livrable sp√©cifique visible depuis l‚Äôespace personnel de l‚Äôutilisateur. "
            "Propose de consulter la plateforme pour plus de d√©tails."
        )

    elif intent_name == "retard":
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "Rassure l‚Äôutilisateur sur son retard. Explique qu‚Äôil peut contacter son coach ou le support pour demander un d√©lai suppl√©mentaire."
        )

    elif intent_name == "prise_de_rdv":
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "Explique comment planifier un rendez-vous ou un appel depuis l‚Äôespace personnel. Propose aussi une suggestion d‚Äôaction simple."
        )

    elif intent_name == "besoin_aide_financement":
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "Pr√©sente bri√®vement les options de financement disponibles dans le programme, et encourage l‚Äôutilisateur √† consulter la section ‚ÄòInvestisseurs‚Äô."
        )

    else:
        # Fallback si l‚Äôintention n‚Äôest pas personnalis√©e
        return (
            f"{base}\n\n"
            f"Message utilisateur : {user_message}\n\n"
            "R√©ponds de fa√ßon bienveillante et utile, avec un ton amical."
        )

async def stream_generator(final_prompt: str):
    MAX_RETRIES = 2
    TIMEOUT = 30.0

    ERROR_MESSAGES = {
        404: "Service non trouv√©. V√©rifie que Ollama est bien lanc√© sur http://127.0.0.1:11434",
        500: "Erreur interne du mod√®le. Merci de r√©essayer plus tard.",
        503: "Le service IA est temporairement indisponible.",
    }

    for attempt in range(MAX_RETRIES):
        try:
            async with httpx.AsyncClient(timeout=TIMEOUT) as client:
                response = await client.post(
                    "http://127.0.0.1:11434/api/generate",
                    json={
                        "model": "llama3",
                        "prompt": final_prompt,
                        "stream": True,
                        "options": {
                            "num_predict": 400  # ou plus selon besoin
                        }
                    }
                )

                if response.status_code != 200:
                    msg = ERROR_MESSAGES.get(response.status_code, f"Erreur inconnue (HTTP {response.status_code})")
                    logging.error(f"[Ollama HTTP Error] {msg}")
                    yield msg
                    return

                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            data = json.loads(line)
                            part = data.get("response", "")
                            if part:
                                yield part
                                await asyncio.sleep(0.01)
                        except Exception as e:
                            logging.error(f"[Stream Parsing Error] {e}")
                            yield "‚ö†Ô∏è Erreur de lecture dans le flux."
                            return
                return

        except httpx.ReadTimeout:
            logging.error("[Timeout] Temps d'attente d√©pass√© pour la r√©ponse du mod√®le.")
            yield "‚è±Ô∏è Temps d'attente d√©pass√©. R√©essaie plus tard."
            return

        except Exception as e:
            logging.error(f"[Tentative {attempt + 1}] ‚ùå Erreur Ollama : {type(e).__name__} - {e}")

    yield "üö´ Toutes les tentatives ont √©chou√©. R√©essaie plus tard."





@app.post("/intentions/train")
async def retrain_intentions(payload: dict = Body(...)):
    try:
        training_text = payload.get("training_text", "").strip()

        # 1. Si vide, on retourne une erreur
        if not training_text:
            return JSONResponse(status_code=400, content={"error": "Aucun texte fourni."})

        # 2. Ajouter au fichier intentions.txt
        with open("intentions/intentions.txt", "a", encoding="utf-8") as f:
            f.write(f"\n{training_text}\n")

        # 3. Recalculer les embeddings
        global intentions_db
        try:
            intentions_db = create_intentions_db()
        except Exception as e:
            logging.error(f"[Startup Error] Impossible de cr√©er intentions_db : {e}")
            intentions_db = None

        logging.info("[Intentions] Intentions mises √† jour avec succ√®s.")
        return {"status": "success", "message": "Base des intentions mise √† jour."}

    except Exception as e:
        logging.error(f"[Intentions Reload ERROR] {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})

try:
    intentions_db = create_intentions_db()
except Exception as e:
    logging.error(f"[Startup Error] Impossible de cr√©er intentions_db : {e}")
    intentions_db = None

# @app.get("/ping")
# async def ping():
#     return {"status": "ok"}

@app.get("/ping")
async def ping():
    try:
        async with httpx.AsyncClient(timeout=2) as client:
            response = await client.get("http://127.0.0.1:11434/")
            if response.status_code == 200:
                return {"status": "ok", "ollama": "online"}
    except:
        pass
    return {"status": "ok", "ollama": "offline"}
