# ==== core/stream.py ====
import httpx, json, asyncio, logging, traceback
import os

from dotenv import load_dotenv  
load_dotenv()  

OLLAMA_API = os.getenv("OLLAMA_API", "http://127.0.0.1:11434")
MODEL_NAME = os.getenv("LLM_MODEL", "llama2:7b")

async def stream_generator(prompt: str):
    for attempt in range(2):  # ‚úÖ Retry une fois
        try:
            timeout = httpx.Timeout(60.0, connect=10.0)
            async with httpx.AsyncClient(timeout=timeout) as client:
                resp = await client.post(
                    f"{OLLAMA_API}/api/generate",
                    json={
                        "model": MODEL_NAME,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.5,
                            "stop": ["</s>"]
                        }
                    }
                )

                if resp.status_code != 200:
                    yield "‚ö†Ô∏è Le serveur IA est indisponible. R√©essaye plus tard."
                    return

                data = resp.json()
                if "response" in data:
                    yield data["response"]
                else:
                    yield "‚ö†Ô∏è Aucune r√©ponse g√©n√©r√©e."

        except httpx.TimeoutException:
            logging.error("[Timeout] Le backend IA a mis trop de temps √† r√©pondre.")
            yield "‚ö†Ô∏è D√©lai d√©pass√©. Le bot a mis trop de temps √† r√©pondre."
            return

        except Exception as e:
            logging.error(f"[Stream Error] {type(e).__name__}: {e}")
            yield "‚ö†Ô∏è Une erreur inattendue est survenue."
            return

    yield "üö´ √âchec IA. R√©essaye plus tard."

